{"cells": [{"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(project_id='ae1a755d-e162-4f07-9f5a-130d2280e78e', project_access_token='p-aa90b9b21de435c3f4c94494a24b5c5e69d030f8')\npc = project.project_context\n", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Extract the Personal Identifiable Information (PII) using Watson NLP"}, {"metadata": {}, "cell_type": "markdown", "source": "<h2>Use Case</h2>\n\nThis notebook demonstrates how to extract PII entities using Watson NLP Pertained models also demonstrates how to prepare custom train models. PII (Personally extraction is the process of identifying and extracting personal information from a document or dataset. This information can include names, addresses, phone numbers, email addresses, Social Security numbers, Credit Card number, and other types of information that can be used to identify an individual. \n\n\n<h2>What you'll learn in this notebook</h2>\n\nWatson NLP offers Pertained Models for various NLP tasks also provides fine-tune functionality for custom training. This notebooks shows:\n\n* <b>RBR</b>:  A Rule-Based Reasoner (RBR) in NLP works by using a set of predefined rules to process and understand natural language input. These rules are used to identify specific patterns or structures in the input text and determine the meaning of the text based on those patterns.\n\n\n* <b>BILSTM</b>: the BiLSTM network would take the preprocessed text as input and learn to identify patterns and relationships between words that are indicative of PII data. The BiLSTM network would then output a probability score for each word in the text, indicating the likelihood that the word is part of a PII entity. The BiLSTM network may also be trained to recognize specific entities such as names, addresses, phone numbers, email addresses, etc.\n\n\n* <b>SIRE</b>: Statistical Information and Relation Extraction (SIRE) is a technique used in natural language processing (NLP) to extract specific information and relationships from text. It involves using machine learning algorithms to identify and extract structured data such as entities, attributes, and relations from unstructured text. SIRE is used in a variety of applications, including information extraction, knowledge graph construction, and question answering. SIRE typically uses supervised learning approach, where a model is trained using annotated examples of text and the corresponding structured data. The model can then be used to extract the same information from new, unseen text.\n\n\n* <b>Bert</b>: IBM Watson NLP BERT uses a pre-trained version of BERT that was trained on a large corpus of text data. The pre-trained model can be fine-tuned on a specific task such as text classification, named entity recognition, and more. The BERT architecture consists of an encoder network that is made up of multiple layers of transformer blocks. Each transformer block includes a self-attention mechanism and a feed-forward neural network.\n\n\n* <b>Transformer</b>: This model is a neural network architecture that is used in natural language processing tasks such as language translation, text summarization, and language generation. It is based on self-attention mechanism and can be used to extract information such as named entities, relationships and sentiments from the text."}, {"metadata": {}, "cell_type": "markdown", "source": "## Table of Contents\n\n\n1.  [Before you start](#beforeYouStart)\n1.\t[Load Entity PII Models](#LoadModel)\n1.  [Load PII XLSX Dataset from Data Assets](#Loaddata)\n1.  [TrainingData](#TrainingData)\n1.  [Watson NLP Models](#NLPModels)    \n    1. [BiLSTM Fine-tuned](#BILSTMFINE)\n    1. [SIRE Fine-tuned](#SIRETune)\n    1. [Transformer Fine-tuned](#TransTUne)\n    \n1.  [Testing With Hanzo's Test Dataset](#Testing)    \n1.  [Summary](#summary)"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"beforeYouStart\"></a>\n### 1. Before you start\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<div class=\"alert alert-block alert-danger\">\n<b>Stop kernel of other notebooks.</b></div>\n\n**Note:** If you have other notebooks currently running with the _Default Python 3.8 + Watson NLP XS_ environment, **stop their kernels** before running this notebook. All these notebooks share the same runtime environment, and if they are running in parallel, you may encounter memory issues. To stop the kernel of another notebook, open that notebook, and select _File > Stop Kernel_.\n\n<div class=\"alert alert-block alert-warning\">\n<b>Set Project token.</b></div>\n\nBefore you can begin working on this notebook in Watson Studio in Cloud Pak for Data as a Service, you need to ensure that the project token is set so that you can access the project assets via the notebook.\n\nWhen this notebook is added to the project, a project access token should be inserted at the top of the notebook in a code cell. If you do not see the cell above, add the token to the notebook by clicking **More > Insert project token** from the notebook action bar.  By running the inserted hidden code cell, a project object is created that you can use to access project resources.\n\n![ws-project.mov](https://media.giphy.com/media/jSVxX2spqwWF9unYrs/giphy.gif)\n\n<div class=\"alert alert-block alert-info\">\n<b>Tip:</b> Cell execution</div>\n\nNote that you can step through the notebook execution cell by cell, by selecting Shift-Enter. Or you can execute the entire notebook by selecting **Cell -> Run All** from the menu."}, {"metadata": {}, "cell_type": "code", "source": "import json\nimport pandas as pd\nimport watson_nlp\nfrom watson_nlp import data_model as dm\nfrom watson_nlp.toolkit.entity_mentions_utils import prepare_train_from_json", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Silence Tensorflow warnings\nimport tensorflow as tf\ntf.get_logger().setLevel('ERROR')\ntf.autograph.set_verbosity(0)", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"LoadModel\"></a>\n### 2. Load Entity PII Models"}, {"metadata": {}, "cell_type": "code", "source": "# Load a syntax model to split the text into sentences and tokens\nsyntax_model = watson_nlp.load(watson_nlp.download('syntax_izumo_en_stock'))\n# Load bilstm model in WatsonNLP\nbilstm_model = watson_nlp.load(watson_nlp.download('entity-mentions_bilstm_en_pii'))\n# Load rbr model in WatsonNLP\nrbr_model = watson_nlp.load(watson_nlp.download('entity-mentions_rbr_multi_pii'))\n# Download the GloVe model to be used as embeddings in the BiLSTM\nglove_model = watson_nlp.load(watson_nlp.download('embedding_glove_en_stock'))\n# Download the algorithm template\nmentions_train_template = watson_nlp.load(watson_nlp.download('file_path_entity-mentions_sire_multi_template-crf'))\n# Download the feature extractor\ndefault_feature_extractor = watson_nlp.load(watson_nlp.download('feature-extractor_rbr_entity-mentions_sire_en_stock'))\n# Download and load the pretrained model resource\n#pretrained_model_resource = watson_nlp.load(watson_nlp.download('pretrained-model_roberta-base_v2-8-0_llm_transformer_lang_en_cased_2022-05-06-052653'))", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"Loaddata\"></a>\n### 3. Load PII XLSX Dataset from Data Assets"}, {"metadata": {}, "cell_type": "code", "source": "import os, types\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\ncos_client = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='o0avUc3SDky2d6pNzjuewCSTPPX7tQNz6BKKvL37nBL3',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.private.us.cloud-object-storage.appdomain.cloud')\n\nbucket = 'watsoncore-donotdelete-pr-olkxvfa8bk0pb1'\nobject_key = '10-MB-Test.xlsx'\n\nbody = cos_client.get_object(Bucket=bucket,Key=object_key)['Body']\n\ndf = pd.read_excel(body.read())\ndf = df.dropna()\ndf.head()", "execution_count": 12, "outputs": [{"output_type": "execute_result", "execution_count": 12, "data": {"text/plain": "  First and Last Name          SSN   Credit Card Number First and Last Name.1  \\\n1       Robert\u00a0Aragon  489-36-8350  4929-3813-3266-4295         Robert\u00a0Aragon   \n2       Ashley\u00a0Borden  514-14-8905  5370-4638-8881-3020         Ashley\u00a0Borden   \n3       Thomas\u00a0Conley  690-05-5315  4916-4811-5814-8111         Thomas\u00a0Conley   \n4         Susan\u00a0Davis  421-37-1396  4916-4034-9269-8783           Susan\u00a0Davis   \n5    Christopher\u00a0Diaz  458-02-6124  5299-1561-5689-1938      Christopher\u00a0Diaz   \n\n         SSN.1 Credit Card Number.1 First and Last Name.2        SSN.2  \\\n1  489-36-8351  4929-3813-3266-4296         Robert\u00a0Aragon  489-36-8352   \n2  514-14-8906  5370-4638-8881-3021         Ashley\u00a0Borden  514-14-8907   \n3  690-05-5316  4916-4811-5814-8112         Thomas\u00a0Conley  690-05-5317   \n4  421-37-1397  4916-4034-9269-8784           Susan\u00a0Davis  421-37-1398   \n5  458-02-6125  5299-1561-5689-1939      Christopher\u00a0Diaz  458-02-6126   \n\n  Credit Card Number.2 First and Last Name.3  ... Credit Card Number.3  \\\n1  4929-3813-3266-4297         Robert\u00a0Aragon  ...  4929-3813-3266-4298   \n2  5370-4638-8881-3022         Ashley\u00a0Borden  ...  5370-4638-8881-3023   \n3  4916-4811-5814-8113         Thomas\u00a0Conley  ...  4916-4811-5814-8114   \n4  4916-4034-9269-8785           Susan\u00a0Davis  ...  4916-4034-9269-8786   \n5  5299-1561-5689-1940      Christopher\u00a0Diaz  ...  5299-1561-5689-1941   \n\n  First and Last Name.4        SSN.4 Credit Card Number.4  \\\n1         Robert\u00a0Aragon  489-36-8354  4929-3813-3266-4299   \n2         Ashley\u00a0Borden  514-14-8909  5370-4638-8881-3024   \n3         Thomas\u00a0Conley  690-05-5319  4916-4811-5814-8115   \n4           Susan\u00a0Davis  421-37-1400  4916-4034-9269-8787   \n5      Christopher\u00a0Diaz  458-02-6128  5299-1561-5689-1942   \n\n  First and Last Name.5        SSN.5 Credit Card Number.5  \\\n1         Robert\u00a0Aragon  489-36-8355  4929-3813-3266-4300   \n2         Ashley\u00a0Borden  514-14-8910  5370-4638-8881-3025   \n3         Thomas\u00a0Conley  690-05-5320  4916-4811-5814-8116   \n4           Susan\u00a0Davis  421-37-1401  4916-4034-9269-8788   \n5      Christopher\u00a0Diaz  458-02-6129  5299-1561-5689-1943   \n\n  First and Last Name.6        SSN.6 Credit Card Number.6  \n1         Robert\u00a0Aragon  489-36-8355  4929-3813-3266-4300  \n2         Ashley\u00a0Borden  514-14-8910  5370-4638-8881-3025  \n3         Thomas\u00a0Conley  690-05-5320  4916-4811-5814-8116  \n4           Susan\u00a0Davis  421-37-1401  4916-4034-9269-8788  \n5      Christopher\u00a0Diaz  458-02-6129  5299-1561-5689-1943  \n\n[5 rows x 21 columns]", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>First and Last Name</th>\n      <th>SSN</th>\n      <th>Credit Card Number</th>\n      <th>First and Last Name.1</th>\n      <th>SSN.1</th>\n      <th>Credit Card Number.1</th>\n      <th>First and Last Name.2</th>\n      <th>SSN.2</th>\n      <th>Credit Card Number.2</th>\n      <th>First and Last Name.3</th>\n      <th>...</th>\n      <th>Credit Card Number.3</th>\n      <th>First and Last Name.4</th>\n      <th>SSN.4</th>\n      <th>Credit Card Number.4</th>\n      <th>First and Last Name.5</th>\n      <th>SSN.5</th>\n      <th>Credit Card Number.5</th>\n      <th>First and Last Name.6</th>\n      <th>SSN.6</th>\n      <th>Credit Card Number.6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Robert\u00a0Aragon</td>\n      <td>489-36-8350</td>\n      <td>4929-3813-3266-4295</td>\n      <td>Robert\u00a0Aragon</td>\n      <td>489-36-8351</td>\n      <td>4929-3813-3266-4296</td>\n      <td>Robert\u00a0Aragon</td>\n      <td>489-36-8352</td>\n      <td>4929-3813-3266-4297</td>\n      <td>Robert\u00a0Aragon</td>\n      <td>...</td>\n      <td>4929-3813-3266-4298</td>\n      <td>Robert\u00a0Aragon</td>\n      <td>489-36-8354</td>\n      <td>4929-3813-3266-4299</td>\n      <td>Robert\u00a0Aragon</td>\n      <td>489-36-8355</td>\n      <td>4929-3813-3266-4300</td>\n      <td>Robert\u00a0Aragon</td>\n      <td>489-36-8355</td>\n      <td>4929-3813-3266-4300</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ashley\u00a0Borden</td>\n      <td>514-14-8905</td>\n      <td>5370-4638-8881-3020</td>\n      <td>Ashley\u00a0Borden</td>\n      <td>514-14-8906</td>\n      <td>5370-4638-8881-3021</td>\n      <td>Ashley\u00a0Borden</td>\n      <td>514-14-8907</td>\n      <td>5370-4638-8881-3022</td>\n      <td>Ashley\u00a0Borden</td>\n      <td>...</td>\n      <td>5370-4638-8881-3023</td>\n      <td>Ashley\u00a0Borden</td>\n      <td>514-14-8909</td>\n      <td>5370-4638-8881-3024</td>\n      <td>Ashley\u00a0Borden</td>\n      <td>514-14-8910</td>\n      <td>5370-4638-8881-3025</td>\n      <td>Ashley\u00a0Borden</td>\n      <td>514-14-8910</td>\n      <td>5370-4638-8881-3025</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Thomas\u00a0Conley</td>\n      <td>690-05-5315</td>\n      <td>4916-4811-5814-8111</td>\n      <td>Thomas\u00a0Conley</td>\n      <td>690-05-5316</td>\n      <td>4916-4811-5814-8112</td>\n      <td>Thomas\u00a0Conley</td>\n      <td>690-05-5317</td>\n      <td>4916-4811-5814-8113</td>\n      <td>Thomas\u00a0Conley</td>\n      <td>...</td>\n      <td>4916-4811-5814-8114</td>\n      <td>Thomas\u00a0Conley</td>\n      <td>690-05-5319</td>\n      <td>4916-4811-5814-8115</td>\n      <td>Thomas\u00a0Conley</td>\n      <td>690-05-5320</td>\n      <td>4916-4811-5814-8116</td>\n      <td>Thomas\u00a0Conley</td>\n      <td>690-05-5320</td>\n      <td>4916-4811-5814-8116</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Susan\u00a0Davis</td>\n      <td>421-37-1396</td>\n      <td>4916-4034-9269-8783</td>\n      <td>Susan\u00a0Davis</td>\n      <td>421-37-1397</td>\n      <td>4916-4034-9269-8784</td>\n      <td>Susan\u00a0Davis</td>\n      <td>421-37-1398</td>\n      <td>4916-4034-9269-8785</td>\n      <td>Susan\u00a0Davis</td>\n      <td>...</td>\n      <td>4916-4034-9269-8786</td>\n      <td>Susan\u00a0Davis</td>\n      <td>421-37-1400</td>\n      <td>4916-4034-9269-8787</td>\n      <td>Susan\u00a0Davis</td>\n      <td>421-37-1401</td>\n      <td>4916-4034-9269-8788</td>\n      <td>Susan\u00a0Davis</td>\n      <td>421-37-1401</td>\n      <td>4916-4034-9269-8788</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Christopher\u00a0Diaz</td>\n      <td>458-02-6124</td>\n      <td>5299-1561-5689-1938</td>\n      <td>Christopher\u00a0Diaz</td>\n      <td>458-02-6125</td>\n      <td>5299-1561-5689-1939</td>\n      <td>Christopher\u00a0Diaz</td>\n      <td>458-02-6126</td>\n      <td>5299-1561-5689-1940</td>\n      <td>Christopher\u00a0Diaz</td>\n      <td>...</td>\n      <td>5299-1561-5689-1941</td>\n      <td>Christopher\u00a0Diaz</td>\n      <td>458-02-6128</td>\n      <td>5299-1561-5689-1942</td>\n      <td>Christopher\u00a0Diaz</td>\n      <td>458-02-6129</td>\n      <td>5299-1561-5689-1943</td>\n      <td>Christopher\u00a0Diaz</td>\n      <td>458-02-6129</td>\n      <td>5299-1561-5689-1943</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 21 columns</p>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"TrainingData\"></a>\n### 4. Preparing Training Data"}, {"metadata": {}, "cell_type": "markdown", "source": "Let's generate sentences using the columns of PII information. Ideally, the sentences would include name, SSN, and credit card number in context."}, {"metadata": {}, "cell_type": "code", "source": "def format_data(df, name_col, ssn_col, ccn_col):  \n    import random\n    \n    train_list = []\n    for i in range(1, len(df)):\n        name = df[name_col][i] \n        ssn = str(df[ssn_col][i])\n        ccn = str(df[ccn_col][i])\n        \n        text1 = \"My name is %s, and my social security number is %s. Here's the number to my Visa credit card, %s\" % (name, ssn, ccn)\n        text2 = \"%s is my social security number. The name on my American Express card %s is %s.\" % (ssn, ccn, name)\n        text3 = \"\"\n        text = random.choice([text1, text2])\n\n        name_begin = text.find(name)\n        name_end = text.find(name) + len(name)\n        ssn_begin = text.find(ssn)\n        ssn_end = text.find(ssn) + len(ssn)\n        ccn_begin = text.find(ccn)\n        ccn_end = text.find(ccn) + len(ccn)\n\n        data = {\n                    \"text\": text,\n                    \"mentions\": [\n                        {\n                            \"location\": {\n                                \"begin\": name_begin,\n                                \"end\": name_end\n                            },\n                            \"text\": name,\n                            \"type\": \"Name\"\n                        },\n                        {\n                            \"location\": {\n                                \"begin\": ssn_begin,\n                                \"end\": ssn_end\n                            },\n                            \"text\": ssn,\n                            \"type\": \"SocialSecurityNumber\"\n                        },\n                        {\n                            \"location\": {\n                                \"begin\": ccn_begin,\n                                \"end\": ccn_end\n                            },\n                            \"text\": ccn,\n                            \"type\": \"CreditCardNumber\"\n                        }\n                    ]   \n                }\n\n        train_list.append(data)\n    return train_list", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "train_list = format_data(df=df, name_col='First and Last Name', ssn_col='SSN', ccn_col='Credit Card Number')", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Save the sentences into a json training file and a json dev file. This will save the file to the runtime local as well as the project data assets."}, {"metadata": {}, "cell_type": "code", "source": "with open('PII_text_train.json', 'w') as f:\n    json.dump(train_list, f)\nproject.save_data('PII_text_train.json', data=json.dumps(train_list), overwrite=True)", "execution_count": 15, "outputs": [{"output_type": "execute_result", "execution_count": 15, "data": {"text/plain": "{'file_name': 'PII_text_train.json',\n 'message': 'File saved to project storage.',\n 'bucket_name': 'watsoncore-donotdelete-pr-olkxvfa8bk0pb1',\n 'asset_id': '216b85be-aabe-4ff6-b264-acd101222fbc'}"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "dev_list = format_data(df=df, name_col='First and Last Name.1', ssn_col='SSN.1', ccn_col='Credit Card Number.1')", "execution_count": 16, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "with open('PII_text_dev.json', 'w') as f:\n    json.dump(dev_list, f)\nproject.save_data('PII_text_dev.json', data=json.dumps(dev_list), overwrite=True)", "execution_count": 17, "outputs": [{"output_type": "execute_result", "execution_count": 17, "data": {"text/plain": "{'file_name': 'PII_text_dev.json',\n 'message': 'File saved to project storage.',\n 'bucket_name': 'watsoncore-donotdelete-pr-olkxvfa8bk0pb1',\n 'asset_id': '76834e31-ab93-4aca-b86b-ce6e71476478'}"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "text = \"My name is %s, and my social security number is %s. Here's the number to my Visa credit card, %s\" % (df['First and Last Name'][1], df['SSN'][1], df['Credit Card Number'][1])", "execution_count": 18, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "train_data = dm.DataStream.from_json_array(\"PII_text_train.json\")\ntrain_iob_stream = prepare_train_from_json(train_data, syntax_model)\ndev_data = dm.DataStream.from_json_array(\"PII_text_dev.json\")\ndev_iob_stream = prepare_train_from_json(dev_data, syntax_model)", "execution_count": 19, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"NLPModels\"></a>\n### 5. Watson NLP Models"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"BILSTMFINE\"></a>\n\n### BiLSTM Fine-tuned"}, {"metadata": {}, "cell_type": "code", "source": "bilstm_custom = bilstm_model.train(train_iob_stream, \n                                   dev_iob_stream, \n                                   embedding=glove_model.embedding,\n                                   #vocab_tags=None, \n                                   #char_embed_dim=32, \n                                   #dropout=0.2, \n                                   #num_oov_buckets=1, \n                                   num_train_epochs=5,\n                                   num_conf_epochs=5, \n                                   checkpoint_interval=5, \n                                   learning_rate=0.005, \n                                   #shuffle_buffer=2000, \n                                   #char_lstm_size=64, \n                                   #char_bidir=False, \n                                   lstm_size=16, \n                                   #train_batch_size=32, \n                                   #lower_case=False, \n                                   #embedding_lowercase=True, \n                                   #keep_model_artifacts=False)\n                                  )", "execution_count": null, "outputs": [{"output_type": "stream", "text": "2066/2138 [===========================>..] - ETA: 6s - loss: 1.7969e-04", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "project.save_data('bilstm_pii_custom', data=bilstm_custom.as_file_like_object(), overwrite=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "syntax_result = syntax_model.run(text)\nbilstm_result = bilstm_custom.run(syntax_result)\nbilstm_result", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"SIRETune\"></a>\n\n### SIRE Fine-tuned\n"}, {"metadata": {}, "cell_type": "code", "source": "help(watson_nlp.blocks.entity_mentions.SIRE)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sire_custom = watson_nlp.blocks.entity_mentions.SIRE.train(train_iob_stream, \n                                                           'en', \n                                                           mentions_train_template,\n                                                           feature_extractors=[default_feature_extractor])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "project.save_data('sire_pii_custom', data=sire_custom.as_file_like_object(), overwrite=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "syntax_result = syntax_model.run(text)\nsire_result = sire_custom.run(syntax_result)\nsire_result", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"TransTUne\"></a>\n### Transformer Fine-tuned\n"}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "#help(watson_nlp.blocks.entity_mentions.Transformer)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Download and load the pretrained model resource\npretrained_model_resource = watson_nlp.load(watson_nlp.download('pretrained-model_watbert_multi_transformer_multi_uncased'))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "'''\ntransformer_custom = watson_nlp.blocks.entity_mentions.Transformer.train(train_iob_stream,\n                                                                         dev_iob_stream,\n                                                                         pretrained_model_resource,\n                                                                         lr=0.005,\n                                                                         num_train_epochs=5,\n                                                                         #per_device_train_batch_size=32,\n                                                                         #per_device_eval_batch_size=32,\n                                                                         max_seq_length=205,\n                                                                         seed=1,\n                                                                         keep_model_artifacts=True)\n'''\ntransformer_custom = watson_nlp.blocks.entity_mentions.Transformer.train(train_iob_stream,\n                                                                         dev_iob_stream,\n                                                                         pretrained_model_resource,\n                                                                         num_train_epochs=8,\n                                                                         learning_rate=3e-5,\n                                                                         per_device_train_batch_size=1,\n                                                                         per_device_eval_batch_size=32)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "project.save_data('transformer_pii_custom', data=transformer_custom.as_file_like_object(), overwrite=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Test the custom train transformer model \nsyntax_result = syntax_model.run(test1)\ntransformer_result = transformer_custom.run(syntax_result)\ntransformer_result", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"summary\"></a>\n## 6. Summary\n\n<span style=\"color:blue\">This notebook shows you how to use the Watson NLP library to:\n1. Extract PII Using Custom or Fine tune Models </span>"}, {"metadata": {}, "cell_type": "markdown", "source": "Please note that this content is made available by IBM Build Lab to foster Embedded AI technology adoption. The content may include systems & methods pending patent with USPTO and protected under US Patent Laws. For redistribution of this content, IBM will use release process. For any questions please log an issue in the GitHub.\n\nDeveloped by IBM Build Lab\n\nCopyright - 2022 IBM Corporation"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.6", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}